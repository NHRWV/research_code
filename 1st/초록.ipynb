{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682a2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570ecd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.read_excel('all_df.xlsx')\n",
    "before_covid_umap_df = pd.read_excel('before_covid_umap_df(20) (2).xlsx')\n",
    "covid_umap_df = pd.read_excel('covid_umap_df(20) (2).xlsx')\n",
    "after_covid_umap_df = pd.read_excel('after_covid_umap_df(20) (2).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94468a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(before_covid_umap_df[before_covid_umap_df[\"cluster\"] == 1][\"초록(국문)\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e611620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=\"\"\n",
    ")\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401239a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "당신은 관광 분야 연구의 동향을 분석해야 합니다.\n",
    "연구보고서의 초록들이 주어지면, 초록들의 내용을 관통하는 주제나 소재를 담아서 간략하게 요약하는 한국어 문장을 생성하는 것이 당신의 임무입니다.\n",
    "\n",
    "초록 : {sentences}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentences\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sum_up_lst2 = [[], [], []] # 프롬프트\n",
    "\n",
    "df_lst = [before_covid_umap_df, covid_umap_df, after_covid_umap_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1bcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(df_lst):\n",
    "    cluster_num = 0\n",
    "    \n",
    "    while cluster_num in i['cluster'].values:\n",
    "        result = ''\n",
    "        \n",
    "        for k in range(0, 30, 15):\n",
    "            target = i[i['cluster'] == cluster_num]['성과명'][k:k+15]\n",
    "            \n",
    "            if not target.empty:\n",
    "                result += openai(prompt_template.format(\n",
    "                    sentences = list(target)\n",
    "                ))\n",
    "            \n",
    "        sum_up_lst2[idx].append(result)\n",
    "        \n",
    "        cluster_num += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73987d58",
   "metadata": {},
   "source": [
    "## 맵리듀스 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44369027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c9ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# # Map\n",
    "# map_template = \"\"\"The following is a set of documents\n",
    "# {docs}\n",
    "# Based on this list of docs, please identify the main themes \n",
    "# Helpful Answer:\"\"\"\n",
    "# map_prompt = PromptTemplate.from_template(map_template)\n",
    "# map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "\n",
    "llm = OpenAI(\n",
    "#    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=\"\"\n",
    ")\n",
    "\n",
    "# map_template = \"\"\"\\\n",
    "# 당신은 관광 분야 연구의 동향을 요약해야 합니다.\n",
    "# 연구보고서의 초록들이 주어지면, 초록들의 내용을 관통하는 주제나 소재를 담아서 간략하게 요약하는 한국어 문장을 생성하는 것이 당신의 임무입니다.\n",
    "\n",
    "# 초록 : {sentences}\n",
    "# \"\"\"\n",
    "\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes in Korean \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=map_template\n",
    ")\n",
    "\n",
    "map_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d22aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "# reduce_template = \"\"\"The following is set of summaries:\n",
    "# {docs}\n",
    "# Take these and distill it into a final, consolidated summary of the main themes. \n",
    "# Helpful Answer:\"\"\"\n",
    "\n",
    "# reduce_template = \"\"\"다음은 요약문들을 모은 집합입니다:\n",
    "# {sentences}\n",
    "# 이것들을 합쳐서 한글로 하나의 최종 요약 결과물을 최대 300글자로 간결하게 완성된 문장으로 만들어주세요:\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"The following is set of summaries in Korean:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary in Korean of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5e36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "#    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b15cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=1000, chunk_overlap=0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45201e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b9c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddac8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_up_lst2 = [[], [], []] # 프롬프트\n",
    "\n",
    "df_lst = [before_covid_umap_df, covid_umap_df, after_covid_umap_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d386893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(df_lst):\n",
    "    cluster_num = 0\n",
    "    \n",
    "    while cluster_num in i['cluster'].values:\n",
    "        split_docs = text_splitter.create_documents(i[i['cluster'] == cluster_num]['초록(국문)'])\n",
    "\n",
    "        result = map_reduce_chain.run(split_docs)    \n",
    "        sum_up_lst2[idx].append(result)\n",
    "        \n",
    "        cluster_num += 1\n",
    "        \n",
    "        \n",
    "# test_text = text_splitter.create_documents(df_lst[0]['초록(국문)'][:15])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_up_lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f63c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum_up_lst2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5a822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(map_reduce_chain.run(split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2316556",
   "metadata": {},
   "source": [
    "## load_summarize_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e27a55e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['sentences'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26680\\256191382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m chain = load_summarize_chain(llm=llm, \n\u001b[0m\u001b[0;32m     33\u001b[0m                              \u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                              \u001b[0mcombine_prompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_prompt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\summarize\\__init__.py\u001b[0m in \u001b[0;36mload_summarize_chain\u001b[1;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         )\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\summarize\\__init__.py\u001b[0m in \u001b[0;36m_load_map_reduce_chain\u001b[1;34m(llm, map_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m# TODO: document prompt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     combine_documents_chain = StuffDocumentsChain(\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mllm_chain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_chain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mdocument_variable_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_document_variable_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\load\\serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\main.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['sentences'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=\"\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\\\n",
    "당신은 관광 분야 연구의 동향을 분석해야 합니다.\n",
    "연구보고서의 초록들이 주어지면, 초록들의 내용을 관통하는 주제나 소재를 담아서 간략하게 요약하는 한국어 문장을 생성하는 것이 당신의 임무입니다.\n",
    "\n",
    "초록 : {sentences}\n",
    "\"\"\"\n",
    "\n",
    "combine_template = \"\"\"요약 문장들을 모은 집합을 합쳐서 한글로 하나의 최종 요약 결과물을 만들어주세요\n",
    "요약 문장 집합 : {sentences}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentences\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_template, input_variables=['sentences'])\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, \n",
    "                             prompt=prompt_template, \n",
    "                             combine_prompt=combine_prompt, \n",
    "                             chain_type=\"map_reduce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_up_lst2 = [[], [], []] # 프롬프트\n",
    "\n",
    "df_lst = [before_covid_umap_df, covid_umap_df, after_covid_umap_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000)\n",
    "\n",
    "for idx, i in enumerate(df_lst):\n",
    "    cluster_num = 0\n",
    "    \n",
    "    while cluster_num in i['cluster'].values:\n",
    "        split_docs = text_splitter.split_documents(i[i['cluster'] == cluster_num]['초록(국문)'])\n",
    "        \n",
    "        result = chain.run(split_docs)    \n",
    "        sum_up_lst2[idx].append(result)\n",
    "        \n",
    "        cluster_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbd764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
